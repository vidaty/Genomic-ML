{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHFGANu4aJLU"
      },
      "source": [
        "# Project description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D63dF9vbaJLW"
      },
      "source": [
        "Gene expression profile is the measurement of the activity (the expression) of thousands of genes in a sample. The internal patterns of the GEPs such as coexpression of large clusters of genes suggest that the dimensionality of GEPs can be significantly reduced. I am going to use a variety of dimension reduction techniques to see if this can really be accomplished, and to compare these techniques' suitability for this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lmjJTDVaJLX"
      },
      "source": [
        "For the purpose of this project, I consider the reconstruction mean squared error as the measure to compare different methods. In other words, For each method, I first reduce the dimension and then try to reconstruct the input using the reduced data. Finally I compute MSE between the reconstructed and original data and use it as the measure to compare methods for the task at hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THShoNWPaJLY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.layers import Input, Dense, Lambda, Dropout, BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras import optimizers, regularizers, callbacks\n",
        "from keras.activations import sigmoid, softplus\n",
        "import tensorflow as tf\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ1aoJjbaJLZ"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VnSnwysaJLa"
      },
      "source": [
        "`data.txt` contains normalized expression profiles of 20184 genes in 1040 samples of 16 different human tissues and cell types from different datasets of NCBI GEO, which were collected and preprocessed in CellNet. `SampleCellTypes.txt` contains the type of the cell samples present in `data.txt`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVG_w1QGaJLa"
      },
      "source": [
        "Load the data using the command below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fmkYFy1aJLb"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"data.txt\", header=None)\n",
        "labels = pd.read_csv(\"SampleCellTypes.txt\", header=None)\n",
        "labels = [label[0] for label in labels.values]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69S_rakfaJLb"
      },
      "source": [
        "# Cell Type Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "k7lqevUyaJLc",
        "outputId": "02f538e5-49d5-48f6-94ed-df839ebdbe30"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(15, 8))\n",
        "sns.countplot(x=labels)\n",
        "plt.xlabel('Cell Type');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPssn5h7aJLd"
      },
      "source": [
        "Number of samples from each cell type in the dataset is always the same. Therefore, I can use a measure like accuracy to evaluate the classification performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc9oOzHSaJLd"
      },
      "source": [
        "# Variable selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sL2UiExaJLd"
      },
      "source": [
        "To reduce the computation costs, instead of working on all 20184 genes present in the dataset, I will focus on 1000 genes that have the most variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCFODYfpaJLe"
      },
      "outputs": [],
      "source": [
        "limited_data = data[data.var().sort_values(ascending=False)[:1000].index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LK17aRQaJLe"
      },
      "source": [
        "# Data preperation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wXNWAZ7aJLf"
      },
      "source": [
        "Let's split the data to train and test with a ratio of 3:1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4waHSgDaJLf"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    limited_data, \n",
        "    labels, \n",
        "    test_size=0.25,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTI5CBbzaJLg"
      },
      "source": [
        "# Dimension Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAhEHcNWaJLg"
      },
      "source": [
        "I will use a latent space of dimension 30 for all the dimension reduction methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOYMs8v9aJLh"
      },
      "outputs": [],
      "source": [
        "latent_space_dim = 30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0K9x3nlaJLh"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd8fy7UNaJLh"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=latent_space_dim)\n",
        "pca.fit(X_train)\n",
        "\n",
        "\n",
        "pca_train = pca.transform(X_train)\n",
        "pca_test = pca.transform(X_test)\n",
        "\n",
        "reconstructed_train = pca.inverse_transform(pca_train)\n",
        "reconstructed_test = pca.inverse_transform(pca_test)\n",
        "\n",
        "\n",
        "train_pca_error = mean_squared_error(X_train, reconstructed_train) # Mean Squared Error\n",
        "test_pca_error = mean_squared_error(X_test, reconstructed_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kjgs638BaJLi",
        "outputId": "7b98e615-8762-4c20-a7e6-0c0fe4056530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train MSE: 0.355\n",
            "Test MSE: 0.411\n"
          ]
        }
      ],
      "source": [
        "print(f'Train MSE: {round(train_pca_error, 3)}\\nTest MSE: {round(test_pca_error, 3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeqKeb2QaJLi"
      },
      "source": [
        "## Classification over PCA space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7NtaZ3taJLj",
        "outputId": "7b18f20a-65b1-44f0-9658-f5b3e21dafda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "clf.fit(pca_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hkj9Ka_YaJLj",
        "outputId": "756e63eb-38c6-4447-eee2-f29ff96edc0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9884615384615385"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "np.mean(clf.predict(pca_test) == y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper's Auto Encoder"
      ],
      "metadata": {
        "id": "Hd-8xQKp38cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = Input(shape=(1000,))\n",
        "encoded = input_data\n",
        "encoded = Dense(300)(encoded)\n",
        "encoded = Lambda(lambda x: tf.math.log(sigmoid(x) + 1e-10))(encoded)\n",
        "encoded = Dense(30)(encoded)\n",
        "\n",
        "decoded = encoded\n",
        "decoded = Dense(300)(decoded)\n",
        "decoded = Lambda(lambda x: tf.math.log(sigmoid(x) + 1e-10))(decoded)\n",
        "decoded = Dense(1000)(decoded)"
      ],
      "metadata": {
        "id": "zIEP7S2r4DwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder = Model(input_data, decoded)\n",
        "sgd = optimizers.SGD(lr=0.02)\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                        patience=5, min_lr=0.001)\n",
        "autoencoder.compile(optimizer=sgd, loss='mean_squared_error')\n",
        "\n",
        "history = autoencoder.fit(X_train.to_numpy(), X_train.to_numpy(),\n",
        "                epochs=200,\n",
        "                batch_size=5,\n",
        "                callbacks=[reduce_lr],\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test.to_numpy(), X_test.to_numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RNbppTM4Iyr",
        "outputId": "d6148f87-03ea-4205-f1e2-46e6e3b6cfd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "156/156 [==============================] - 3s 12ms/step - loss: 5.3987 - val_loss: 3.8016 - lr: 0.0200\n",
            "Epoch 2/200\n",
            "156/156 [==============================] - 2s 10ms/step - loss: 3.1025 - val_loss: 2.5547 - lr: 0.0200\n",
            "Epoch 3/200\n",
            "156/156 [==============================] - 1s 9ms/step - loss: 2.2414 - val_loss: 1.9955 - lr: 0.0200\n",
            "Epoch 4/200\n",
            "156/156 [==============================] - 2s 11ms/step - loss: 1.8945 - val_loss: 1.7542 - lr: 0.0200\n",
            "Epoch 5/200\n",
            "156/156 [==============================] - 2s 11ms/step - loss: 1.6349 - val_loss: 1.6416 - lr: 0.0200\n",
            "Epoch 6/200\n",
            "156/156 [==============================] - 2s 11ms/step - loss: 1.4995 - val_loss: 1.4243 - lr: 0.0200\n",
            "Epoch 7/200\n",
            "156/156 [==============================] - 2s 10ms/step - loss: 1.3721 - val_loss: 1.3174 - lr: 0.0200\n",
            "Epoch 8/200\n",
            "156/156 [==============================] - 1s 9ms/step - loss: 1.2692 - val_loss: 1.3066 - lr: 0.0200\n",
            "Epoch 9/200\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 1.1818 - val_loss: 1.1426 - lr: 0.0200\n",
            "Epoch 10/200\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 1.1077 - val_loss: 1.0907 - lr: 0.0200\n",
            "Epoch 11/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 1.0230 - val_loss: 1.0749 - lr: 0.0200\n",
            "Epoch 12/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.9705 - val_loss: 0.9801 - lr: 0.0200\n",
            "Epoch 13/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.9183 - val_loss: 0.9719 - lr: 0.0200\n",
            "Epoch 14/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.8831 - val_loss: 0.9140 - lr: 0.0200\n",
            "Epoch 15/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.8511 - val_loss: 0.8426 - lr: 0.0200\n",
            "Epoch 16/200\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.8097 - val_loss: 0.8306 - lr: 0.0200\n",
            "Epoch 17/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.7827 - val_loss: 0.7889 - lr: 0.0200\n",
            "Epoch 18/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.7490 - val_loss: 0.7714 - lr: 0.0200\n",
            "Epoch 19/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.7309 - val_loss: 0.7628 - lr: 0.0200\n",
            "Epoch 20/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.7156 - val_loss: 0.7532 - lr: 0.0200\n",
            "Epoch 21/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.6923 - val_loss: 0.7436 - lr: 0.0200\n",
            "Epoch 22/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.6827 - val_loss: 0.7024 - lr: 0.0200\n",
            "Epoch 23/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.6622 - val_loss: 0.7018 - lr: 0.0200\n",
            "Epoch 24/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.6592 - val_loss: 0.7471 - lr: 0.0200\n",
            "Epoch 25/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.6376 - val_loss: 0.7085 - lr: 0.0200\n",
            "Epoch 26/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.6302 - val_loss: 0.6680 - lr: 0.0200\n",
            "Epoch 27/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.6258 - val_loss: 0.6777 - lr: 0.0200\n",
            "Epoch 28/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.6124 - val_loss: 0.6683 - lr: 0.0200\n",
            "Epoch 29/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.6107 - val_loss: 0.6527 - lr: 0.0200\n",
            "Epoch 30/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.6013 - val_loss: 0.6611 - lr: 0.0200\n",
            "Epoch 31/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5954 - val_loss: 0.6555 - lr: 0.0200\n",
            "Epoch 32/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5946 - val_loss: 0.6279 - lr: 0.0200\n",
            "Epoch 33/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5850 - val_loss: 0.6647 - lr: 0.0200\n",
            "Epoch 34/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5766 - val_loss: 0.6277 - lr: 0.0200\n",
            "Epoch 35/200\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.5731 - val_loss: 0.6328 - lr: 0.0200\n",
            "Epoch 36/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5712 - val_loss: 0.6232 - lr: 0.0200\n",
            "Epoch 37/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5590 - val_loss: 0.6173 - lr: 0.0200\n",
            "Epoch 38/200\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.5563 - val_loss: 0.5997 - lr: 0.0200\n",
            "Epoch 39/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5497 - val_loss: 0.5964 - lr: 0.0200\n",
            "Epoch 40/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5440 - val_loss: 0.5943 - lr: 0.0200\n",
            "Epoch 41/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5405 - val_loss: 0.5888 - lr: 0.0200\n",
            "Epoch 42/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5394 - val_loss: 0.5938 - lr: 0.0200\n",
            "Epoch 43/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5322 - val_loss: 0.5844 - lr: 0.0200\n",
            "Epoch 44/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5306 - val_loss: 0.5877 - lr: 0.0200\n",
            "Epoch 45/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5284 - val_loss: 0.5759 - lr: 0.0200\n",
            "Epoch 46/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5193 - val_loss: 0.5775 - lr: 0.0200\n",
            "Epoch 47/200\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.5146 - val_loss: 0.5633 - lr: 0.0200\n",
            "Epoch 48/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5101 - val_loss: 0.5854 - lr: 0.0200\n",
            "Epoch 49/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5078 - val_loss: 0.5804 - lr: 0.0200\n",
            "Epoch 50/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5038 - val_loss: 0.5675 - lr: 0.0200\n",
            "Epoch 51/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.5001 - val_loss: 0.5733 - lr: 0.0200\n",
            "Epoch 52/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4963 - val_loss: 0.5677 - lr: 0.0200\n",
            "Epoch 53/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4668 - val_loss: 0.5361 - lr: 0.0040\n",
            "Epoch 54/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4646 - val_loss: 0.5356 - lr: 0.0040\n",
            "Epoch 55/200\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4637 - val_loss: 0.5339 - lr: 0.0040\n",
            "Epoch 56/200\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4631 - val_loss: 0.5329 - lr: 0.0040\n",
            "Epoch 57/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4621 - val_loss: 0.5321 - lr: 0.0040\n",
            "Epoch 58/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4616 - val_loss: 0.5316 - lr: 0.0040\n",
            "Epoch 59/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4608 - val_loss: 0.5311 - lr: 0.0040\n",
            "Epoch 60/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4601 - val_loss: 0.5308 - lr: 0.0040\n",
            "Epoch 61/200\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4598 - val_loss: 0.5298 - lr: 0.0040\n",
            "Epoch 62/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4591 - val_loss: 0.5290 - lr: 0.0040\n",
            "Epoch 63/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4583 - val_loss: 0.5294 - lr: 0.0040\n",
            "Epoch 64/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4578 - val_loss: 0.5281 - lr: 0.0040\n",
            "Epoch 65/200\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4570 - val_loss: 0.5282 - lr: 0.0040\n",
            "Epoch 66/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4564 - val_loss: 0.5274 - lr: 0.0040\n",
            "Epoch 67/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4559 - val_loss: 0.5268 - lr: 0.0040\n",
            "Epoch 68/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4553 - val_loss: 0.5259 - lr: 0.0040\n",
            "Epoch 69/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4547 - val_loss: 0.5252 - lr: 0.0040\n",
            "Epoch 70/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4542 - val_loss: 0.5248 - lr: 0.0040\n",
            "Epoch 71/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4534 - val_loss: 0.5242 - lr: 0.0040\n",
            "Epoch 72/200\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4529 - val_loss: 0.5243 - lr: 0.0040\n",
            "Epoch 73/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4523 - val_loss: 0.5234 - lr: 0.0040\n",
            "Epoch 74/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4515 - val_loss: 0.5229 - lr: 0.0040\n",
            "Epoch 75/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4510 - val_loss: 0.5224 - lr: 0.0040\n",
            "Epoch 76/200\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4506 - val_loss: 0.5215 - lr: 0.0040\n",
            "Epoch 77/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4499 - val_loss: 0.5218 - lr: 0.0040\n",
            "Epoch 78/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4493 - val_loss: 0.5211 - lr: 0.0040\n",
            "Epoch 79/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4486 - val_loss: 0.5200 - lr: 0.0040\n",
            "Epoch 80/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4481 - val_loss: 0.5201 - lr: 0.0040\n",
            "Epoch 81/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4475 - val_loss: 0.5194 - lr: 0.0040\n",
            "Epoch 82/200\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4471 - val_loss: 0.5185 - lr: 0.0040\n",
            "Epoch 83/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4463 - val_loss: 0.5178 - lr: 0.0040\n",
            "Epoch 84/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4459 - val_loss: 0.5179 - lr: 0.0040\n",
            "Epoch 85/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4453 - val_loss: 0.5169 - lr: 0.0040\n",
            "Epoch 86/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4447 - val_loss: 0.5165 - lr: 0.0040\n",
            "Epoch 87/200\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4441 - val_loss: 0.5162 - lr: 0.0040\n",
            "Epoch 88/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4435 - val_loss: 0.5165 - lr: 0.0040\n",
            "Epoch 89/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4431 - val_loss: 0.5152 - lr: 0.0040\n",
            "Epoch 90/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4424 - val_loss: 0.5145 - lr: 0.0040\n",
            "Epoch 91/200\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4421 - val_loss: 0.5141 - lr: 0.0040\n",
            "Epoch 92/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4413 - val_loss: 0.5135 - lr: 0.0040\n",
            "Epoch 93/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4409 - val_loss: 0.5127 - lr: 0.0040\n",
            "Epoch 94/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4403 - val_loss: 0.5124 - lr: 0.0040\n",
            "Epoch 95/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4399 - val_loss: 0.5115 - lr: 0.0040\n",
            "Epoch 96/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4391 - val_loss: 0.5122 - lr: 0.0040\n",
            "Epoch 97/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4388 - val_loss: 0.5111 - lr: 0.0040\n",
            "Epoch 98/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4383 - val_loss: 0.5109 - lr: 0.0040\n",
            "Epoch 99/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4376 - val_loss: 0.5097 - lr: 0.0040\n",
            "Epoch 100/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4370 - val_loss: 0.5093 - lr: 0.0040\n",
            "Epoch 101/200\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4366 - val_loss: 0.5083 - lr: 0.0040\n",
            "Epoch 102/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4361 - val_loss: 0.5084 - lr: 0.0040\n",
            "Epoch 103/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4357 - val_loss: 0.5078 - lr: 0.0040\n",
            "Epoch 104/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4350 - val_loss: 0.5081 - lr: 0.0040\n",
            "Epoch 105/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4345 - val_loss: 0.5071 - lr: 0.0040\n",
            "Epoch 106/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4340 - val_loss: 0.5072 - lr: 0.0040\n",
            "Epoch 107/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4336 - val_loss: 0.5061 - lr: 0.0040\n",
            "Epoch 108/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4331 - val_loss: 0.5054 - lr: 0.0040\n",
            "Epoch 109/200\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4324 - val_loss: 0.5044 - lr: 0.0040\n",
            "Epoch 110/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4320 - val_loss: 0.5051 - lr: 0.0040\n",
            "Epoch 111/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4314 - val_loss: 0.5038 - lr: 0.0040\n",
            "Epoch 112/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4309 - val_loss: 0.5044 - lr: 0.0040\n",
            "Epoch 113/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4305 - val_loss: 0.5037 - lr: 0.0040\n",
            "Epoch 114/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4301 - val_loss: 0.5025 - lr: 0.0040\n",
            "Epoch 115/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4295 - val_loss: 0.5021 - lr: 0.0040\n",
            "Epoch 116/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4291 - val_loss: 0.5017 - lr: 0.0040\n",
            "Epoch 117/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4285 - val_loss: 0.5015 - lr: 0.0040\n",
            "Epoch 118/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4281 - val_loss: 0.5018 - lr: 0.0040\n",
            "Epoch 119/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4275 - val_loss: 0.5003 - lr: 0.0040\n",
            "Epoch 120/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4270 - val_loss: 0.5002 - lr: 0.0040\n",
            "Epoch 121/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4266 - val_loss: 0.4995 - lr: 0.0040\n",
            "Epoch 122/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4260 - val_loss: 0.5004 - lr: 0.0040\n",
            "Epoch 123/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4256 - val_loss: 0.4993 - lr: 0.0040\n",
            "Epoch 124/200\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4251 - val_loss: 0.4983 - lr: 0.0040\n",
            "Epoch 125/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4246 - val_loss: 0.4985 - lr: 0.0040\n",
            "Epoch 126/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4242 - val_loss: 0.4980 - lr: 0.0040\n",
            "Epoch 127/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4237 - val_loss: 0.4979 - lr: 0.0040\n",
            "Epoch 128/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4231 - val_loss: 0.4965 - lr: 0.0040\n",
            "Epoch 129/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4228 - val_loss: 0.4962 - lr: 0.0040\n",
            "Epoch 130/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4222 - val_loss: 0.4963 - lr: 0.0040\n",
            "Epoch 131/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4218 - val_loss: 0.4952 - lr: 0.0040\n",
            "Epoch 132/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4213 - val_loss: 0.4956 - lr: 0.0040\n",
            "Epoch 133/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4209 - val_loss: 0.4946 - lr: 0.0040\n",
            "Epoch 134/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4206 - val_loss: 0.4940 - lr: 0.0040\n",
            "Epoch 135/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4200 - val_loss: 0.4936 - lr: 0.0040\n",
            "Epoch 136/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4196 - val_loss: 0.4933 - lr: 0.0040\n",
            "Epoch 137/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4192 - val_loss: 0.4928 - lr: 0.0040\n",
            "Epoch 138/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4186 - val_loss: 0.4929 - lr: 0.0040\n",
            "Epoch 139/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4182 - val_loss: 0.4922 - lr: 0.0040\n",
            "Epoch 140/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4178 - val_loss: 0.4918 - lr: 0.0040\n",
            "Epoch 141/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4173 - val_loss: 0.4910 - lr: 0.0040\n",
            "Epoch 142/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4166 - val_loss: 0.4913 - lr: 0.0040\n",
            "Epoch 143/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4163 - val_loss: 0.4903 - lr: 0.0040\n",
            "Epoch 144/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4158 - val_loss: 0.4897 - lr: 0.0040\n",
            "Epoch 145/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4155 - val_loss: 0.4897 - lr: 0.0040\n",
            "Epoch 146/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4150 - val_loss: 0.4889 - lr: 0.0040\n",
            "Epoch 147/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4145 - val_loss: 0.4890 - lr: 0.0040\n",
            "Epoch 148/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4142 - val_loss: 0.4884 - lr: 0.0040\n",
            "Epoch 149/200\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4137 - val_loss: 0.4876 - lr: 0.0040\n",
            "Epoch 150/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4132 - val_loss: 0.4871 - lr: 0.0040\n",
            "Epoch 151/200\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4129 - val_loss: 0.4879 - lr: 0.0040\n",
            "Epoch 152/200\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4123 - val_loss: 0.4864 - lr: 0.0040\n",
            "Epoch 153/200\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4120 - val_loss: 0.4863 - lr: 0.0040\n",
            "Epoch 154/200\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4116 - val_loss: 0.4862 - lr: 0.0040\n",
            "Epoch 155/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4112 - val_loss: 0.4854 - lr: 0.0040\n",
            "Epoch 156/200\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4106 - val_loss: 0.4856 - lr: 0.0040\n",
            "Epoch 157/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4103 - val_loss: 0.4849 - lr: 0.0040\n",
            "Epoch 158/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4099 - val_loss: 0.4841 - lr: 0.0040\n",
            "Epoch 159/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4095 - val_loss: 0.4838 - lr: 0.0040\n",
            "Epoch 160/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4089 - val_loss: 0.4833 - lr: 0.0040\n",
            "Epoch 161/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4086 - val_loss: 0.4830 - lr: 0.0040\n",
            "Epoch 162/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4080 - val_loss: 0.4828 - lr: 0.0040\n",
            "Epoch 163/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4077 - val_loss: 0.4823 - lr: 0.0040\n",
            "Epoch 164/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4073 - val_loss: 0.4820 - lr: 0.0040\n",
            "Epoch 165/200\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4069 - val_loss: 0.4815 - lr: 0.0040\n",
            "Epoch 166/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4065 - val_loss: 0.4808 - lr: 0.0040\n",
            "Epoch 167/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4062 - val_loss: 0.4806 - lr: 0.0040\n",
            "Epoch 168/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4056 - val_loss: 0.4810 - lr: 0.0040\n",
            "Epoch 169/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4052 - val_loss: 0.4801 - lr: 0.0040\n",
            "Epoch 170/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4048 - val_loss: 0.4795 - lr: 0.0040\n",
            "Epoch 171/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4045 - val_loss: 0.4797 - lr: 0.0040\n",
            "Epoch 172/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4040 - val_loss: 0.4783 - lr: 0.0040\n",
            "Epoch 173/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4035 - val_loss: 0.4784 - lr: 0.0040\n",
            "Epoch 174/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4031 - val_loss: 0.4790 - lr: 0.0040\n",
            "Epoch 175/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4027 - val_loss: 0.4777 - lr: 0.0040\n",
            "Epoch 176/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4023 - val_loss: 0.4771 - lr: 0.0040\n",
            "Epoch 177/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4019 - val_loss: 0.4767 - lr: 0.0040\n",
            "Epoch 178/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4014 - val_loss: 0.4770 - lr: 0.0040\n",
            "Epoch 179/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4012 - val_loss: 0.4758 - lr: 0.0040\n",
            "Epoch 180/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4006 - val_loss: 0.4762 - lr: 0.0040\n",
            "Epoch 181/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4003 - val_loss: 0.4756 - lr: 0.0040\n",
            "Epoch 182/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3999 - val_loss: 0.4754 - lr: 0.0040\n",
            "Epoch 183/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3997 - val_loss: 0.4749 - lr: 0.0040\n",
            "Epoch 184/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3991 - val_loss: 0.4755 - lr: 0.0040\n",
            "Epoch 185/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3988 - val_loss: 0.4746 - lr: 0.0040\n",
            "Epoch 186/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3985 - val_loss: 0.4733 - lr: 0.0040\n",
            "Epoch 187/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3980 - val_loss: 0.4733 - lr: 0.0040\n",
            "Epoch 188/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3975 - val_loss: 0.4729 - lr: 0.0040\n",
            "Epoch 189/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3972 - val_loss: 0.4721 - lr: 0.0040\n",
            "Epoch 190/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3968 - val_loss: 0.4724 - lr: 0.0040\n",
            "Epoch 191/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3965 - val_loss: 0.4715 - lr: 0.0040\n",
            "Epoch 192/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3961 - val_loss: 0.4715 - lr: 0.0040\n",
            "Epoch 193/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3957 - val_loss: 0.4718 - lr: 0.0040\n",
            "Epoch 194/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3954 - val_loss: 0.4711 - lr: 0.0040\n",
            "Epoch 195/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3949 - val_loss: 0.4703 - lr: 0.0040\n",
            "Epoch 196/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3946 - val_loss: 0.4697 - lr: 0.0040\n",
            "Epoch 197/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3943 - val_loss: 0.4697 - lr: 0.0040\n",
            "Epoch 198/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3937 - val_loss: 0.4692 - lr: 0.0040\n",
            "Epoch 199/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3933 - val_loss: 0.4685 - lr: 0.0040\n",
            "Epoch 200/200\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.3932 - val_loss: 0.4685 - lr: 0.0040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train MSE is: 0.3932')\n",
        "print('Test MSE is: 0.4685')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCs05mf37Z5X",
        "outputId": "6132e8e8-2557-4960-8bb0-9b872052772f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train MSE is: 0.3932\n",
            "Test MSE is: 0.4685\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "BBnJteDDjI9c",
        "outputId": "89ebb51f-f870-4de4-da10-686de732df76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv6B5PU-aJLk"
      },
      "source": [
        "# Deeper Auto Encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "734kV7IMaJLk"
      },
      "outputs": [],
      "source": [
        "input_data = Input(shape=(1000,))\n",
        "encoded = input_data\n",
        "encoded = Dense(400)(encoded)\n",
        "encoded = Lambda(lambda x: tf.math.log(sigmoid(x) + 1e-10))(encoded)\n",
        "encoded = Dense(100)(encoded)\n",
        "encoded = Lambda(lambda x: tf.math.log(sigmoid(x) + 1e-10))(encoded)\n",
        "encoded = Dense(30)(encoded)\n",
        "\n",
        "decoded = encoded\n",
        "decoded = Dense(100)(decoded)\n",
        "decoded = Lambda(lambda x: tf.math.log(sigmoid(x) + 1e-10))(decoded)\n",
        "decoded = Dense(400)(decoded)\n",
        "decoded = Lambda(lambda x: tf.math.log(sigmoid(x) + 1e-10))(decoded)\n",
        "decoded = Dense(1000)(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX78T_cJaJLk",
        "outputId": "7f648897-ca4d-462e-fdf8-805ede0b14c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "156/156 [==============================] - 3s 12ms/step - loss: 2.7615 - val_loss: 2.3400 - lr: 0.0200\n",
            "Epoch 2/300\n",
            "156/156 [==============================] - 2s 12ms/step - loss: 2.1288 - val_loss: 2.0089 - lr: 0.0200\n",
            "Epoch 3/300\n",
            "156/156 [==============================] - 1s 9ms/step - loss: 1.8794 - val_loss: 1.7406 - lr: 0.0200\n",
            "Epoch 4/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 1.7222 - val_loss: 1.7346 - lr: 0.0200\n",
            "Epoch 5/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 1.5548 - val_loss: 1.7010 - lr: 0.0200\n",
            "Epoch 6/300\n",
            "156/156 [==============================] - 2s 10ms/step - loss: 1.4611 - val_loss: 1.3891 - lr: 0.0200\n",
            "Epoch 7/300\n",
            "156/156 [==============================] - 2s 13ms/step - loss: 1.3668 - val_loss: 1.3101 - lr: 0.0200\n",
            "Epoch 8/300\n",
            "156/156 [==============================] - 2s 12ms/step - loss: 1.2838 - val_loss: 1.2322 - lr: 0.0200\n",
            "Epoch 9/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 1.1956 - val_loss: 1.1709 - lr: 0.0200\n",
            "Epoch 10/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 1.1039 - val_loss: 1.0772 - lr: 0.0200\n",
            "Epoch 11/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 1.0411 - val_loss: 1.1121 - lr: 0.0200\n",
            "Epoch 12/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.9678 - val_loss: 1.1483 - lr: 0.0200\n",
            "Epoch 13/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.9210 - val_loss: 0.9571 - lr: 0.0200\n",
            "Epoch 14/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.9002 - val_loss: 0.9676 - lr: 0.0200\n",
            "Epoch 15/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.8672 - val_loss: 0.8756 - lr: 0.0200\n",
            "Epoch 16/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.8091 - val_loss: 0.8370 - lr: 0.0200\n",
            "Epoch 17/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.7898 - val_loss: 0.9019 - lr: 0.0200\n",
            "Epoch 18/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.7715 - val_loss: 0.8097 - lr: 0.0200\n",
            "Epoch 19/300\n",
            "156/156 [==============================] - 1s 9ms/step - loss: 0.7463 - val_loss: 0.8074 - lr: 0.0200\n",
            "Epoch 20/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.7239 - val_loss: 0.7749 - lr: 0.0200\n",
            "Epoch 21/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.7123 - val_loss: 0.7736 - lr: 0.0200\n",
            "Epoch 22/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.6994 - val_loss: 0.7582 - lr: 0.0200\n",
            "Epoch 23/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.6840 - val_loss: 0.7252 - lr: 0.0200\n",
            "Epoch 24/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.6800 - val_loss: 0.7281 - lr: 0.0200\n",
            "Epoch 25/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.6595 - val_loss: 0.7395 - lr: 0.0200\n",
            "Epoch 26/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.6504 - val_loss: 0.6942 - lr: 0.0200\n",
            "Epoch 27/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.6418 - val_loss: 0.6963 - lr: 0.0200\n",
            "Epoch 28/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.6509 - val_loss: 0.6820 - lr: 0.0200\n",
            "Epoch 29/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.6258 - val_loss: 0.7448 - lr: 0.0200\n",
            "Epoch 30/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.6230 - val_loss: 0.6595 - lr: 0.0200\n",
            "Epoch 31/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.6159 - val_loss: 0.6583 - lr: 0.0200\n",
            "Epoch 32/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.6070 - val_loss: 0.6714 - lr: 0.0200\n",
            "Epoch 33/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.6100 - val_loss: 0.6437 - lr: 0.0200\n",
            "Epoch 34/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5950 - val_loss: 0.6583 - lr: 0.0200\n",
            "Epoch 35/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.5961 - val_loss: 0.6592 - lr: 0.0200\n",
            "Epoch 36/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5813 - val_loss: 0.6386 - lr: 0.0200\n",
            "Epoch 37/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5895 - val_loss: 0.6315 - lr: 0.0200\n",
            "Epoch 38/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5845 - val_loss: 0.7005 - lr: 0.0200\n",
            "Epoch 39/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5702 - val_loss: 0.6334 - lr: 0.0200\n",
            "Epoch 40/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5701 - val_loss: 0.6552 - lr: 0.0200\n",
            "Epoch 41/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5681 - val_loss: 0.6363 - lr: 0.0200\n",
            "Epoch 42/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5580 - val_loss: 0.6419 - lr: 0.0200\n",
            "Epoch 43/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.5221 - val_loss: 0.5920 - lr: 0.0040\n",
            "Epoch 44/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.5185 - val_loss: 0.5904 - lr: 0.0040\n",
            "Epoch 45/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.5172 - val_loss: 0.5901 - lr: 0.0040\n",
            "Epoch 46/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5160 - val_loss: 0.5899 - lr: 0.0040\n",
            "Epoch 47/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5151 - val_loss: 0.5878 - lr: 0.0040\n",
            "Epoch 48/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.5144 - val_loss: 0.5867 - lr: 0.0040\n",
            "Epoch 49/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.5133 - val_loss: 0.5859 - lr: 0.0040\n",
            "Epoch 50/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5126 - val_loss: 0.5858 - lr: 0.0040\n",
            "Epoch 51/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5115 - val_loss: 0.5853 - lr: 0.0040\n",
            "Epoch 52/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5108 - val_loss: 0.5835 - lr: 0.0040\n",
            "Epoch 53/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.5097 - val_loss: 0.5837 - lr: 0.0040\n",
            "Epoch 54/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.5090 - val_loss: 0.5822 - lr: 0.0040\n",
            "Epoch 55/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5083 - val_loss: 0.5802 - lr: 0.0040\n",
            "Epoch 56/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5074 - val_loss: 0.5809 - lr: 0.0040\n",
            "Epoch 57/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5066 - val_loss: 0.5801 - lr: 0.0040\n",
            "Epoch 58/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.5057 - val_loss: 0.5790 - lr: 0.0040\n",
            "Epoch 59/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5048 - val_loss: 0.5787 - lr: 0.0040\n",
            "Epoch 60/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5042 - val_loss: 0.5774 - lr: 0.0040\n",
            "Epoch 61/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5033 - val_loss: 0.5766 - lr: 0.0040\n",
            "Epoch 62/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5025 - val_loss: 0.5762 - lr: 0.0040\n",
            "Epoch 63/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5016 - val_loss: 0.5757 - lr: 0.0040\n",
            "Epoch 64/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5010 - val_loss: 0.5747 - lr: 0.0040\n",
            "Epoch 65/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.5001 - val_loss: 0.5740 - lr: 0.0040\n",
            "Epoch 66/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4995 - val_loss: 0.5722 - lr: 0.0040\n",
            "Epoch 67/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4985 - val_loss: 0.5724 - lr: 0.0040\n",
            "Epoch 68/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4979 - val_loss: 0.5715 - lr: 0.0040\n",
            "Epoch 69/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4969 - val_loss: 0.5716 - lr: 0.0040\n",
            "Epoch 70/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4964 - val_loss: 0.5696 - lr: 0.0040\n",
            "Epoch 71/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4955 - val_loss: 0.5701 - lr: 0.0040\n",
            "Epoch 72/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4948 - val_loss: 0.5685 - lr: 0.0040\n",
            "Epoch 73/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4939 - val_loss: 0.5689 - lr: 0.0040\n",
            "Epoch 74/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4933 - val_loss: 0.5679 - lr: 0.0040\n",
            "Epoch 75/300\n",
            "156/156 [==============================] - 1s 9ms/step - loss: 0.4925 - val_loss: 0.5664 - lr: 0.0040\n",
            "Epoch 76/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4917 - val_loss: 0.5670 - lr: 0.0040\n",
            "Epoch 77/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4911 - val_loss: 0.5660 - lr: 0.0040\n",
            "Epoch 78/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4903 - val_loss: 0.5654 - lr: 0.0040\n",
            "Epoch 79/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4895 - val_loss: 0.5644 - lr: 0.0040\n",
            "Epoch 80/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4890 - val_loss: 0.5636 - lr: 0.0040\n",
            "Epoch 81/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4881 - val_loss: 0.5627 - lr: 0.0040\n",
            "Epoch 82/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4874 - val_loss: 0.5619 - lr: 0.0040\n",
            "Epoch 83/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4868 - val_loss: 0.5613 - lr: 0.0040\n",
            "Epoch 84/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4859 - val_loss: 0.5613 - lr: 0.0040\n",
            "Epoch 85/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4853 - val_loss: 0.5608 - lr: 0.0040\n",
            "Epoch 86/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4845 - val_loss: 0.5596 - lr: 0.0040\n",
            "Epoch 87/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4838 - val_loss: 0.5595 - lr: 0.0040\n",
            "Epoch 88/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4831 - val_loss: 0.5586 - lr: 0.0040\n",
            "Epoch 89/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4825 - val_loss: 0.5583 - lr: 0.0040\n",
            "Epoch 90/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4818 - val_loss: 0.5575 - lr: 0.0040\n",
            "Epoch 91/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4811 - val_loss: 0.5559 - lr: 0.0040\n",
            "Epoch 92/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4803 - val_loss: 0.5570 - lr: 0.0040\n",
            "Epoch 93/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4798 - val_loss: 0.5564 - lr: 0.0040\n",
            "Epoch 94/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4791 - val_loss: 0.5552 - lr: 0.0040\n",
            "Epoch 95/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4784 - val_loss: 0.5544 - lr: 0.0040\n",
            "Epoch 96/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4776 - val_loss: 0.5537 - lr: 0.0040\n",
            "Epoch 97/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4770 - val_loss: 0.5529 - lr: 0.0040\n",
            "Epoch 98/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4764 - val_loss: 0.5524 - lr: 0.0040\n",
            "Epoch 99/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4757 - val_loss: 0.5521 - lr: 0.0040\n",
            "Epoch 100/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4752 - val_loss: 0.5513 - lr: 0.0040\n",
            "Epoch 101/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4744 - val_loss: 0.5507 - lr: 0.0040\n",
            "Epoch 102/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4738 - val_loss: 0.5500 - lr: 0.0040\n",
            "Epoch 103/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4730 - val_loss: 0.5499 - lr: 0.0040\n",
            "Epoch 104/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4724 - val_loss: 0.5491 - lr: 0.0040\n",
            "Epoch 105/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4718 - val_loss: 0.5486 - lr: 0.0040\n",
            "Epoch 106/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4710 - val_loss: 0.5485 - lr: 0.0040\n",
            "Epoch 107/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4703 - val_loss: 0.5474 - lr: 0.0040\n",
            "Epoch 108/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4698 - val_loss: 0.5468 - lr: 0.0040\n",
            "Epoch 109/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4692 - val_loss: 0.5455 - lr: 0.0040\n",
            "Epoch 110/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4686 - val_loss: 0.5451 - lr: 0.0040\n",
            "Epoch 111/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4682 - val_loss: 0.5448 - lr: 0.0040\n",
            "Epoch 112/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4674 - val_loss: 0.5445 - lr: 0.0040\n",
            "Epoch 113/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4667 - val_loss: 0.5439 - lr: 0.0040\n",
            "Epoch 114/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4661 - val_loss: 0.5432 - lr: 0.0040\n",
            "Epoch 115/300\n",
            "156/156 [==============================] - 2s 10ms/step - loss: 0.4654 - val_loss: 0.5429 - lr: 0.0040\n",
            "Epoch 116/300\n",
            "156/156 [==============================] - 2s 11ms/step - loss: 0.4649 - val_loss: 0.5420 - lr: 0.0040\n",
            "Epoch 117/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4643 - val_loss: 0.5416 - lr: 0.0040\n",
            "Epoch 118/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4635 - val_loss: 0.5406 - lr: 0.0040\n",
            "Epoch 119/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4630 - val_loss: 0.5399 - lr: 0.0040\n",
            "Epoch 120/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4624 - val_loss: 0.5400 - lr: 0.0040\n",
            "Epoch 121/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4618 - val_loss: 0.5391 - lr: 0.0040\n",
            "Epoch 122/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4614 - val_loss: 0.5386 - lr: 0.0040\n",
            "Epoch 123/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4606 - val_loss: 0.5391 - lr: 0.0040\n",
            "Epoch 124/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4601 - val_loss: 0.5381 - lr: 0.0040\n",
            "Epoch 125/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4594 - val_loss: 0.5373 - lr: 0.0040\n",
            "Epoch 126/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4588 - val_loss: 0.5378 - lr: 0.0040\n",
            "Epoch 127/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4583 - val_loss: 0.5362 - lr: 0.0040\n",
            "Epoch 128/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4577 - val_loss: 0.5359 - lr: 0.0040\n",
            "Epoch 129/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4571 - val_loss: 0.5343 - lr: 0.0040\n",
            "Epoch 130/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4567 - val_loss: 0.5358 - lr: 0.0040\n",
            "Epoch 131/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4560 - val_loss: 0.5338 - lr: 0.0040\n",
            "Epoch 132/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4554 - val_loss: 0.5339 - lr: 0.0040\n",
            "Epoch 133/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4548 - val_loss: 0.5341 - lr: 0.0040\n",
            "Epoch 134/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4544 - val_loss: 0.5329 - lr: 0.0040\n",
            "Epoch 135/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4537 - val_loss: 0.5320 - lr: 0.0040\n",
            "Epoch 136/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4530 - val_loss: 0.5322 - lr: 0.0040\n",
            "Epoch 137/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4526 - val_loss: 0.5311 - lr: 0.0040\n",
            "Epoch 138/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4519 - val_loss: 0.5308 - lr: 0.0040\n",
            "Epoch 139/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4515 - val_loss: 0.5295 - lr: 0.0040\n",
            "Epoch 140/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4509 - val_loss: 0.5302 - lr: 0.0040\n",
            "Epoch 141/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4504 - val_loss: 0.5287 - lr: 0.0040\n",
            "Epoch 142/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4497 - val_loss: 0.5290 - lr: 0.0040\n",
            "Epoch 143/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4494 - val_loss: 0.5281 - lr: 0.0040\n",
            "Epoch 144/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4487 - val_loss: 0.5272 - lr: 0.0040\n",
            "Epoch 145/300\n",
            "156/156 [==============================] - 2s 11ms/step - loss: 0.4481 - val_loss: 0.5270 - lr: 0.0040\n",
            "Epoch 146/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4477 - val_loss: 0.5261 - lr: 0.0040\n",
            "Epoch 147/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4470 - val_loss: 0.5268 - lr: 0.0040\n",
            "Epoch 148/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.4466 - val_loss: 0.5257 - lr: 0.0040\n",
            "Epoch 149/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4461 - val_loss: 0.5248 - lr: 0.0040\n",
            "Epoch 150/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4454 - val_loss: 0.5258 - lr: 0.0040\n",
            "Epoch 151/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4448 - val_loss: 0.5242 - lr: 0.0040\n",
            "Epoch 152/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4443 - val_loss: 0.5234 - lr: 0.0040\n",
            "Epoch 153/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4438 - val_loss: 0.5234 - lr: 0.0040\n",
            "Epoch 154/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4433 - val_loss: 0.5226 - lr: 0.0040\n",
            "Epoch 155/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4428 - val_loss: 0.5215 - lr: 0.0040\n",
            "Epoch 156/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4424 - val_loss: 0.5212 - lr: 0.0040\n",
            "Epoch 157/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4417 - val_loss: 0.5215 - lr: 0.0040\n",
            "Epoch 158/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4412 - val_loss: 0.5213 - lr: 0.0040\n",
            "Epoch 159/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4408 - val_loss: 0.5198 - lr: 0.0040\n",
            "Epoch 160/300\n",
            "156/156 [==============================] - 2s 11ms/step - loss: 0.4403 - val_loss: 0.5205 - lr: 0.0040\n",
            "Epoch 161/300\n",
            "156/156 [==============================] - 2s 13ms/step - loss: 0.4398 - val_loss: 0.5198 - lr: 0.0040\n",
            "Epoch 162/300\n",
            "156/156 [==============================] - 1s 9ms/step - loss: 0.4391 - val_loss: 0.5195 - lr: 0.0040\n",
            "Epoch 163/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4388 - val_loss: 0.5190 - lr: 0.0040\n",
            "Epoch 164/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4381 - val_loss: 0.5182 - lr: 0.0040\n",
            "Epoch 165/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4379 - val_loss: 0.5176 - lr: 0.0040\n",
            "Epoch 166/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4371 - val_loss: 0.5171 - lr: 0.0040\n",
            "Epoch 167/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4368 - val_loss: 0.5172 - lr: 0.0040\n",
            "Epoch 168/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4361 - val_loss: 0.5164 - lr: 0.0040\n",
            "Epoch 169/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4358 - val_loss: 0.5153 - lr: 0.0040\n",
            "Epoch 170/300\n",
            "156/156 [==============================] - 2s 10ms/step - loss: 0.4352 - val_loss: 0.5158 - lr: 0.0040\n",
            "Epoch 171/300\n",
            "156/156 [==============================] - 2s 10ms/step - loss: 0.4347 - val_loss: 0.5144 - lr: 0.0040\n",
            "Epoch 172/300\n",
            "156/156 [==============================] - 2s 13ms/step - loss: 0.4343 - val_loss: 0.5134 - lr: 0.0040\n",
            "Epoch 173/300\n",
            "156/156 [==============================] - 2s 10ms/step - loss: 0.4338 - val_loss: 0.5140 - lr: 0.0040\n",
            "Epoch 174/300\n",
            "156/156 [==============================] - 2s 15ms/step - loss: 0.4333 - val_loss: 0.5129 - lr: 0.0040\n",
            "Epoch 175/300\n",
            "156/156 [==============================] - 2s 10ms/step - loss: 0.4328 - val_loss: 0.5131 - lr: 0.0040\n",
            "Epoch 176/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4322 - val_loss: 0.5131 - lr: 0.0040\n",
            "Epoch 177/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4319 - val_loss: 0.5119 - lr: 0.0040\n",
            "Epoch 178/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4314 - val_loss: 0.5111 - lr: 0.0040\n",
            "Epoch 179/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4309 - val_loss: 0.5107 - lr: 0.0040\n",
            "Epoch 180/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4304 - val_loss: 0.5104 - lr: 0.0040\n",
            "Epoch 181/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4300 - val_loss: 0.5112 - lr: 0.0040\n",
            "Epoch 182/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4296 - val_loss: 0.5101 - lr: 0.0040\n",
            "Epoch 183/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4290 - val_loss: 0.5092 - lr: 0.0040\n",
            "Epoch 184/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4286 - val_loss: 0.5087 - lr: 0.0040\n",
            "Epoch 185/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4280 - val_loss: 0.5085 - lr: 0.0040\n",
            "Epoch 186/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4276 - val_loss: 0.5083 - lr: 0.0040\n",
            "Epoch 187/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4273 - val_loss: 0.5072 - lr: 0.0040\n",
            "Epoch 188/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4265 - val_loss: 0.5073 - lr: 0.0040\n",
            "Epoch 189/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4262 - val_loss: 0.5074 - lr: 0.0040\n",
            "Epoch 190/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4258 - val_loss: 0.5067 - lr: 0.0040\n",
            "Epoch 191/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4253 - val_loss: 0.5061 - lr: 0.0040\n",
            "Epoch 192/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4248 - val_loss: 0.5057 - lr: 0.0040\n",
            "Epoch 193/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4243 - val_loss: 0.5052 - lr: 0.0040\n",
            "Epoch 194/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4239 - val_loss: 0.5046 - lr: 0.0040\n",
            "Epoch 195/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4235 - val_loss: 0.5046 - lr: 0.0040\n",
            "Epoch 196/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4229 - val_loss: 0.5040 - lr: 0.0040\n",
            "Epoch 197/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4227 - val_loss: 0.5030 - lr: 0.0040\n",
            "Epoch 198/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4221 - val_loss: 0.5030 - lr: 0.0040\n",
            "Epoch 199/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4216 - val_loss: 0.5031 - lr: 0.0040\n",
            "Epoch 200/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4212 - val_loss: 0.5034 - lr: 0.0040\n",
            "Epoch 201/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4209 - val_loss: 0.5015 - lr: 0.0040\n",
            "Epoch 202/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4202 - val_loss: 0.5009 - lr: 0.0040\n",
            "Epoch 203/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4200 - val_loss: 0.5009 - lr: 0.0040\n",
            "Epoch 204/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4195 - val_loss: 0.4999 - lr: 0.0040\n",
            "Epoch 205/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4191 - val_loss: 0.4999 - lr: 0.0040\n",
            "Epoch 206/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4186 - val_loss: 0.5005 - lr: 0.0040\n",
            "Epoch 207/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4182 - val_loss: 0.4996 - lr: 0.0040\n",
            "Epoch 208/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4178 - val_loss: 0.4992 - lr: 0.0040\n",
            "Epoch 209/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4174 - val_loss: 0.4985 - lr: 0.0040\n",
            "Epoch 210/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4169 - val_loss: 0.4975 - lr: 0.0040\n",
            "Epoch 211/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4165 - val_loss: 0.4982 - lr: 0.0040\n",
            "Epoch 212/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4160 - val_loss: 0.4971 - lr: 0.0040\n",
            "Epoch 213/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4156 - val_loss: 0.4973 - lr: 0.0040\n",
            "Epoch 214/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4153 - val_loss: 0.4968 - lr: 0.0040\n",
            "Epoch 215/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4149 - val_loss: 0.4965 - lr: 0.0040\n",
            "Epoch 216/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4143 - val_loss: 0.4961 - lr: 0.0040\n",
            "Epoch 217/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4139 - val_loss: 0.4954 - lr: 0.0040\n",
            "Epoch 218/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4135 - val_loss: 0.4958 - lr: 0.0040\n",
            "Epoch 219/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4132 - val_loss: 0.4946 - lr: 0.0040\n",
            "Epoch 220/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4127 - val_loss: 0.4939 - lr: 0.0040\n",
            "Epoch 221/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4122 - val_loss: 0.4944 - lr: 0.0040\n",
            "Epoch 222/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4118 - val_loss: 0.4942 - lr: 0.0040\n",
            "Epoch 223/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4116 - val_loss: 0.4932 - lr: 0.0040\n",
            "Epoch 224/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4110 - val_loss: 0.4931 - lr: 0.0040\n",
            "Epoch 225/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4106 - val_loss: 0.4927 - lr: 0.0040\n",
            "Epoch 226/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4102 - val_loss: 0.4919 - lr: 0.0040\n",
            "Epoch 227/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4098 - val_loss: 0.4919 - lr: 0.0040\n",
            "Epoch 228/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4095 - val_loss: 0.4907 - lr: 0.0040\n",
            "Epoch 229/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4091 - val_loss: 0.4913 - lr: 0.0040\n",
            "Epoch 230/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4087 - val_loss: 0.4905 - lr: 0.0040\n",
            "Epoch 231/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4081 - val_loss: 0.4903 - lr: 0.0040\n",
            "Epoch 232/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4076 - val_loss: 0.4897 - lr: 0.0040\n",
            "Epoch 233/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4074 - val_loss: 0.4893 - lr: 0.0040\n",
            "Epoch 234/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4069 - val_loss: 0.4899 - lr: 0.0040\n",
            "Epoch 235/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.4065 - val_loss: 0.4895 - lr: 0.0040\n",
            "Epoch 236/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4063 - val_loss: 0.4885 - lr: 0.0040\n",
            "Epoch 237/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4058 - val_loss: 0.4878 - lr: 0.0040\n",
            "Epoch 238/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4054 - val_loss: 0.4877 - lr: 0.0040\n",
            "Epoch 239/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4051 - val_loss: 0.4869 - lr: 0.0040\n",
            "Epoch 240/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4047 - val_loss: 0.4868 - lr: 0.0040\n",
            "Epoch 241/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4043 - val_loss: 0.4863 - lr: 0.0040\n",
            "Epoch 242/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4039 - val_loss: 0.4856 - lr: 0.0040\n",
            "Epoch 243/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4036 - val_loss: 0.4857 - lr: 0.0040\n",
            "Epoch 244/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4031 - val_loss: 0.4856 - lr: 0.0040\n",
            "Epoch 245/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4027 - val_loss: 0.4852 - lr: 0.0040\n",
            "Epoch 246/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4023 - val_loss: 0.4849 - lr: 0.0040\n",
            "Epoch 247/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4020 - val_loss: 0.4852 - lr: 0.0040\n",
            "Epoch 248/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4017 - val_loss: 0.4837 - lr: 0.0040\n",
            "Epoch 249/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4012 - val_loss: 0.4836 - lr: 0.0040\n",
            "Epoch 250/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4007 - val_loss: 0.4838 - lr: 0.0040\n",
            "Epoch 251/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4005 - val_loss: 0.4835 - lr: 0.0040\n",
            "Epoch 252/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.4002 - val_loss: 0.4834 - lr: 0.0040\n",
            "Epoch 253/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.3996 - val_loss: 0.4827 - lr: 0.0040\n",
            "Epoch 254/300\n",
            "156/156 [==============================] - 1s 10ms/step - loss: 0.3994 - val_loss: 0.4831 - lr: 0.0040\n",
            "Epoch 255/300\n",
            "156/156 [==============================] - 2s 11ms/step - loss: 0.3990 - val_loss: 0.4823 - lr: 0.0040\n",
            "Epoch 256/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.3986 - val_loss: 0.4821 - lr: 0.0040\n",
            "Epoch 257/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3983 - val_loss: 0.4811 - lr: 0.0040\n",
            "Epoch 258/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3979 - val_loss: 0.4808 - lr: 0.0040\n",
            "Epoch 259/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.3975 - val_loss: 0.4809 - lr: 0.0040\n",
            "Epoch 260/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3972 - val_loss: 0.4803 - lr: 0.0040\n",
            "Epoch 261/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3968 - val_loss: 0.4795 - lr: 0.0040\n",
            "Epoch 262/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.3965 - val_loss: 0.4793 - lr: 0.0040\n",
            "Epoch 263/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.3960 - val_loss: 0.4791 - lr: 0.0040\n",
            "Epoch 264/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.3959 - val_loss: 0.4789 - lr: 0.0040\n",
            "Epoch 265/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.3953 - val_loss: 0.4795 - lr: 0.0040\n",
            "Epoch 266/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3950 - val_loss: 0.4783 - lr: 0.0040\n",
            "Epoch 267/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.3946 - val_loss: 0.4780 - lr: 0.0040\n",
            "Epoch 268/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3943 - val_loss: 0.4778 - lr: 0.0040\n",
            "Epoch 269/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3939 - val_loss: 0.4781 - lr: 0.0040\n",
            "Epoch 270/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3936 - val_loss: 0.4765 - lr: 0.0040\n",
            "Epoch 271/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3932 - val_loss: 0.4769 - lr: 0.0040\n",
            "Epoch 272/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.3929 - val_loss: 0.4765 - lr: 0.0040\n",
            "Epoch 273/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.3926 - val_loss: 0.4761 - lr: 0.0040\n",
            "Epoch 274/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3923 - val_loss: 0.4753 - lr: 0.0040\n",
            "Epoch 275/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3918 - val_loss: 0.4753 - lr: 0.0040\n",
            "Epoch 276/300\n",
            "156/156 [==============================] - 2s 12ms/step - loss: 0.3915 - val_loss: 0.4754 - lr: 0.0040\n",
            "Epoch 277/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3912 - val_loss: 0.4745 - lr: 0.0040\n",
            "Epoch 278/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3908 - val_loss: 0.4743 - lr: 0.0040\n",
            "Epoch 279/300\n",
            "156/156 [==============================] - 2s 12ms/step - loss: 0.3904 - val_loss: 0.4739 - lr: 0.0040\n",
            "Epoch 280/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.3901 - val_loss: 0.4738 - lr: 0.0040\n",
            "Epoch 281/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3897 - val_loss: 0.4735 - lr: 0.0040\n",
            "Epoch 282/300\n",
            "156/156 [==============================] - 1s 9ms/step - loss: 0.3894 - val_loss: 0.4733 - lr: 0.0040\n",
            "Epoch 283/300\n",
            "156/156 [==============================] - 1s 9ms/step - loss: 0.3892 - val_loss: 0.4726 - lr: 0.0040\n",
            "Epoch 284/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.3886 - val_loss: 0.4726 - lr: 0.0040\n",
            "Epoch 285/300\n",
            "156/156 [==============================] - 1s 8ms/step - loss: 0.3885 - val_loss: 0.4732 - lr: 0.0040\n",
            "Epoch 286/300\n",
            "156/156 [==============================] - 2s 11ms/step - loss: 0.3880 - val_loss: 0.4719 - lr: 0.0040\n",
            "Epoch 287/300\n",
            "156/156 [==============================] - 2s 10ms/step - loss: 0.3877 - val_loss: 0.4715 - lr: 0.0040\n",
            "Epoch 288/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3874 - val_loss: 0.4711 - lr: 0.0040\n",
            "Epoch 289/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.3872 - val_loss: 0.4714 - lr: 0.0040\n",
            "Epoch 290/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3867 - val_loss: 0.4707 - lr: 0.0040\n",
            "Epoch 291/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3864 - val_loss: 0.4703 - lr: 0.0040\n",
            "Epoch 292/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.3862 - val_loss: 0.4696 - lr: 0.0040\n",
            "Epoch 293/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3857 - val_loss: 0.4698 - lr: 0.0040\n",
            "Epoch 294/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3855 - val_loss: 0.4688 - lr: 0.0040\n",
            "Epoch 295/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3850 - val_loss: 0.4692 - lr: 0.0040\n",
            "Epoch 296/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3848 - val_loss: 0.4692 - lr: 0.0040\n",
            "Epoch 297/300\n",
            "156/156 [==============================] - 1s 6ms/step - loss: 0.3845 - val_loss: 0.4685 - lr: 0.0040\n",
            "Epoch 298/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3840 - val_loss: 0.4686 - lr: 0.0040\n",
            "Epoch 299/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3838 - val_loss: 0.4681 - lr: 0.0040\n",
            "Epoch 300/300\n",
            "156/156 [==============================] - 1s 7ms/step - loss: 0.3836 - val_loss: 0.4676 - lr: 0.0040\n"
          ]
        }
      ],
      "source": [
        "autoencoder = Model(input_data, decoded)\n",
        "sgd = optimizers.SGD(lr=0.02)\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                        patience=5, min_lr=0.001)\n",
        "autoencoder.compile(optimizer=sgd, loss='mean_squared_error')\n",
        "\n",
        "history = autoencoder.fit(X_train.to_numpy(), X_train.to_numpy(),\n",
        "                epochs=300,\n",
        "                batch_size=5,\n",
        "                callbacks=[reduce_lr],\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test.to_numpy(), X_test.to_numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train MSE is: 0.3836')\n",
        "print('Test MSE is: 0.4676')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqD11Z_gb5Zl",
        "outputId": "7e15d7a0-8881-4e60-c806-f6f03e61c258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train MSE is: 0.3836\n",
            "Test MSE is: 0.4676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hVDAM8mklb-4",
        "outputId": "926a3f09-1200-43b9-89b6-20138bb8039e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Less Deep Auto Encoders"
      ],
      "metadata": {
        "id": "JTl1-C__9Oyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = Input(shape=(1000,))\n",
        "encoded = input_data\n",
        "encoded = Dense(100)(encoded)\n",
        "encoded = Lambda(lambda x: tf.math.log(sigmoid(x) + 1e-10))(encoded)\n",
        "encoded = Dense(30)(encoded)\n",
        "\n",
        "decoded = encoded\n",
        "decoded = Dense(100)(decoded)\n",
        "decoded = Lambda(lambda x: tf.math.log(sigmoid(x) + 1e-10))(decoded)\n",
        "decoded = Dense(1000)(decoded)"
      ],
      "metadata": {
        "id": "dBSgIHpgh7JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder = Model(input_data, decoded)\n",
        "sgd = optimizers.SGD(lr=0.02)\n",
        "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                                        patience=5, min_lr=0.001)\n",
        "autoencoder.compile(optimizer=sgd, loss='mean_squared_error')\n",
        "\n",
        "history = autoencoder.fit(X_train.to_numpy(), X_train.to_numpy(),\n",
        "                epochs=300,\n",
        "                batch_size=5,\n",
        "                callbacks=[reduce_lr],\n",
        "                shuffle=True,\n",
        "                validation_data=(X_test.to_numpy(), X_test.to_numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPHx0cX69Yo3",
        "outputId": "e3c08c54-1d07-44d1-96c2-c983aa6def5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 5.3883 - val_loss: 3.9202 - lr: 0.0200\n",
            "Epoch 2/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 3.8321 - val_loss: 3.4769 - lr: 0.0200\n",
            "Epoch 3/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 3.1400 - val_loss: 2.5702 - lr: 0.0200\n",
            "Epoch 4/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 2.4393 - val_loss: 2.1833 - lr: 0.0200\n",
            "Epoch 5/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 2.0273 - val_loss: 3.0672 - lr: 0.0200\n",
            "Epoch 6/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 1.8237 - val_loss: 1.8354 - lr: 0.0200\n",
            "Epoch 7/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 1.7025 - val_loss: 1.6649 - lr: 0.0200\n",
            "Epoch 8/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 1.5736 - val_loss: 1.5142 - lr: 0.0200\n",
            "Epoch 9/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 1.4880 - val_loss: 1.4557 - lr: 0.0200\n",
            "Epoch 10/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 1.4044 - val_loss: 1.4403 - lr: 0.0200\n",
            "Epoch 11/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 1.3336 - val_loss: 1.3421 - lr: 0.0200\n",
            "Epoch 12/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 1.2621 - val_loss: 1.3241 - lr: 0.0200\n",
            "Epoch 13/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 1.2059 - val_loss: 1.1909 - lr: 0.0200\n",
            "Epoch 14/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 1.1465 - val_loss: 1.1474 - lr: 0.0200\n",
            "Epoch 15/300\n",
            "156/156 [==============================] - 1s 9ms/step - loss: 1.0944 - val_loss: 1.1078 - lr: 0.0200\n",
            "Epoch 16/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 1.0433 - val_loss: 1.2830 - lr: 0.0200\n",
            "Epoch 17/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 1.0221 - val_loss: 1.0344 - lr: 0.0200\n",
            "Epoch 18/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.9777 - val_loss: 1.0252 - lr: 0.0200\n",
            "Epoch 19/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.9346 - val_loss: 1.0448 - lr: 0.0200\n",
            "Epoch 20/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.8990 - val_loss: 0.9737 - lr: 0.0200\n",
            "Epoch 21/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.8759 - val_loss: 0.9004 - lr: 0.0200\n",
            "Epoch 22/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.8453 - val_loss: 0.8812 - lr: 0.0200\n",
            "Epoch 23/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.8161 - val_loss: 0.8422 - lr: 0.0200\n",
            "Epoch 24/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.8050 - val_loss: 0.8521 - lr: 0.0200\n",
            "Epoch 25/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.7863 - val_loss: 0.7987 - lr: 0.0200\n",
            "Epoch 26/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.7573 - val_loss: 0.7835 - lr: 0.0200\n",
            "Epoch 27/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.7506 - val_loss: 0.7715 - lr: 0.0200\n",
            "Epoch 28/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.7289 - val_loss: 0.7659 - lr: 0.0200\n",
            "Epoch 29/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.7269 - val_loss: 0.8052 - lr: 0.0200\n",
            "Epoch 30/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.7131 - val_loss: 0.7426 - lr: 0.0200\n",
            "Epoch 31/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.6960 - val_loss: 0.7357 - lr: 0.0200\n",
            "Epoch 32/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.6862 - val_loss: 0.7614 - lr: 0.0200\n",
            "Epoch 33/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.7003 - val_loss: 0.7465 - lr: 0.0200\n",
            "Epoch 34/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.6766 - val_loss: 0.7078 - lr: 0.0200\n",
            "Epoch 35/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.6690 - val_loss: 0.7245 - lr: 0.0200\n",
            "Epoch 36/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.6650 - val_loss: 0.6956 - lr: 0.0200\n",
            "Epoch 37/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.6562 - val_loss: 0.7562 - lr: 0.0200\n",
            "Epoch 38/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.6485 - val_loss: 0.7500 - lr: 0.0200\n",
            "Epoch 39/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.6464 - val_loss: 0.6935 - lr: 0.0200\n",
            "Epoch 40/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.6529 - val_loss: 0.6830 - lr: 0.0200\n",
            "Epoch 41/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.6352 - val_loss: 0.6958 - lr: 0.0200\n",
            "Epoch 42/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.6312 - val_loss: 0.7332 - lr: 0.0200\n",
            "Epoch 43/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.6335 - val_loss: 0.6862 - lr: 0.0200\n",
            "Epoch 44/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.6172 - val_loss: 0.6886 - lr: 0.0200\n",
            "Epoch 45/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.6194 - val_loss: 0.6669 - lr: 0.0200\n",
            "Epoch 46/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.6128 - val_loss: 0.6644 - lr: 0.0200\n",
            "Epoch 47/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.6101 - val_loss: 0.6637 - lr: 0.0200\n",
            "Epoch 48/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.6125 - val_loss: 0.6572 - lr: 0.0200\n",
            "Epoch 49/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.6005 - val_loss: 0.6754 - lr: 0.0200\n",
            "Epoch 50/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.5979 - val_loss: 0.6665 - lr: 0.0200\n",
            "Epoch 51/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.5997 - val_loss: 0.6360 - lr: 0.0200\n",
            "Epoch 52/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.5931 - val_loss: 0.6513 - lr: 0.0200\n",
            "Epoch 53/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.5848 - val_loss: 0.6505 - lr: 0.0200\n",
            "Epoch 54/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.5774 - val_loss: 0.6369 - lr: 0.0200\n",
            "Epoch 55/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.5767 - val_loss: 0.6331 - lr: 0.0200\n",
            "Epoch 56/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.5805 - val_loss: 0.6254 - lr: 0.0200\n",
            "Epoch 57/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.5749 - val_loss: 0.6193 - lr: 0.0200\n",
            "Epoch 58/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.5693 - val_loss: 0.6298 - lr: 0.0200\n",
            "Epoch 59/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.5762 - val_loss: 0.6449 - lr: 0.0200\n",
            "Epoch 60/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.5592 - val_loss: 0.6109 - lr: 0.0200\n",
            "Epoch 61/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.5656 - val_loss: 0.6118 - lr: 0.0200\n",
            "Epoch 62/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.5666 - val_loss: 0.6237 - lr: 0.0200\n",
            "Epoch 63/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.5534 - val_loss: 0.6225 - lr: 0.0200\n",
            "Epoch 64/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.5512 - val_loss: 0.6404 - lr: 0.0200\n",
            "Epoch 65/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.5460 - val_loss: 0.6075 - lr: 0.0200\n",
            "Epoch 66/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.5441 - val_loss: 0.6040 - lr: 0.0200\n",
            "Epoch 67/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.5393 - val_loss: 0.6015 - lr: 0.0200\n",
            "Epoch 68/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.5414 - val_loss: 0.5944 - lr: 0.0200\n",
            "Epoch 69/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.5408 - val_loss: 0.5963 - lr: 0.0200\n",
            "Epoch 70/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.5327 - val_loss: 0.5905 - lr: 0.0200\n",
            "Epoch 71/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.5313 - val_loss: 0.6127 - lr: 0.0200\n",
            "Epoch 72/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.5308 - val_loss: 0.5892 - lr: 0.0200\n",
            "Epoch 73/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.5232 - val_loss: 0.5889 - lr: 0.0200\n",
            "Epoch 74/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.5281 - val_loss: 0.5881 - lr: 0.0200\n",
            "Epoch 75/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.5183 - val_loss: 0.5764 - lr: 0.0200\n",
            "Epoch 76/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.5176 - val_loss: 0.5766 - lr: 0.0200\n",
            "Epoch 77/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.5198 - val_loss: 0.5805 - lr: 0.0200\n",
            "Epoch 78/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.5095 - val_loss: 0.5793 - lr: 0.0200\n",
            "Epoch 79/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.5183 - val_loss: 0.5848 - lr: 0.0200\n",
            "Epoch 80/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.5104 - val_loss: 0.5687 - lr: 0.0200\n",
            "Epoch 81/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.5058 - val_loss: 0.5804 - lr: 0.0200\n",
            "Epoch 82/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.5052 - val_loss: 0.5656 - lr: 0.0200\n",
            "Epoch 83/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.5017 - val_loss: 0.5836 - lr: 0.0200\n",
            "Epoch 84/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4991 - val_loss: 0.5755 - lr: 0.0200\n",
            "Epoch 85/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4968 - val_loss: 0.5663 - lr: 0.0200\n",
            "Epoch 86/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.5002 - val_loss: 0.5667 - lr: 0.0200\n",
            "Epoch 87/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4916 - val_loss: 0.5940 - lr: 0.0200\n",
            "Epoch 88/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4653 - val_loss: 0.5386 - lr: 0.0040\n",
            "Epoch 89/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4629 - val_loss: 0.5385 - lr: 0.0040\n",
            "Epoch 90/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4626 - val_loss: 0.5375 - lr: 0.0040\n",
            "Epoch 91/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4619 - val_loss: 0.5377 - lr: 0.0040\n",
            "Epoch 92/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4615 - val_loss: 0.5374 - lr: 0.0040\n",
            "Epoch 93/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4612 - val_loss: 0.5372 - lr: 0.0040\n",
            "Epoch 94/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4608 - val_loss: 0.5361 - lr: 0.0040\n",
            "Epoch 95/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4601 - val_loss: 0.5359 - lr: 0.0040\n",
            "Epoch 96/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4600 - val_loss: 0.5357 - lr: 0.0040\n",
            "Epoch 97/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4595 - val_loss: 0.5354 - lr: 0.0040\n",
            "Epoch 98/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4591 - val_loss: 0.5348 - lr: 0.0040\n",
            "Epoch 99/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4588 - val_loss: 0.5347 - lr: 0.0040\n",
            "Epoch 100/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4584 - val_loss: 0.5345 - lr: 0.0040\n",
            "Epoch 101/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4580 - val_loss: 0.5334 - lr: 0.0040\n",
            "Epoch 102/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4576 - val_loss: 0.5342 - lr: 0.0040\n",
            "Epoch 103/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4573 - val_loss: 0.5336 - lr: 0.0040\n",
            "Epoch 104/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4570 - val_loss: 0.5319 - lr: 0.0040\n",
            "Epoch 105/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4564 - val_loss: 0.5325 - lr: 0.0040\n",
            "Epoch 106/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4561 - val_loss: 0.5325 - lr: 0.0040\n",
            "Epoch 107/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4558 - val_loss: 0.5319 - lr: 0.0040\n",
            "Epoch 108/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4555 - val_loss: 0.5318 - lr: 0.0040\n",
            "Epoch 109/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4550 - val_loss: 0.5312 - lr: 0.0040\n",
            "Epoch 110/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4548 - val_loss: 0.5313 - lr: 0.0040\n",
            "Epoch 111/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4544 - val_loss: 0.5309 - lr: 0.0040\n",
            "Epoch 112/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4540 - val_loss: 0.5300 - lr: 0.0040\n",
            "Epoch 113/300\n",
            "156/156 [==============================] - 1s 9ms/step - loss: 0.4536 - val_loss: 0.5299 - lr: 0.0040\n",
            "Epoch 114/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4533 - val_loss: 0.5304 - lr: 0.0040\n",
            "Epoch 115/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4531 - val_loss: 0.5294 - lr: 0.0040\n",
            "Epoch 116/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4526 - val_loss: 0.5291 - lr: 0.0040\n",
            "Epoch 117/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4522 - val_loss: 0.5278 - lr: 0.0040\n",
            "Epoch 118/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4520 - val_loss: 0.5273 - lr: 0.0040\n",
            "Epoch 119/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4517 - val_loss: 0.5286 - lr: 0.0040\n",
            "Epoch 120/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4512 - val_loss: 0.5277 - lr: 0.0040\n",
            "Epoch 121/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4508 - val_loss: 0.5273 - lr: 0.0040\n",
            "Epoch 122/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4507 - val_loss: 0.5273 - lr: 0.0040\n",
            "Epoch 123/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4503 - val_loss: 0.5265 - lr: 0.0040\n",
            "Epoch 124/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4499 - val_loss: 0.5263 - lr: 0.0040\n",
            "Epoch 125/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4497 - val_loss: 0.5259 - lr: 0.0040\n",
            "Epoch 126/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4492 - val_loss: 0.5261 - lr: 0.0040\n",
            "Epoch 127/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4489 - val_loss: 0.5258 - lr: 0.0040\n",
            "Epoch 128/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4487 - val_loss: 0.5257 - lr: 0.0040\n",
            "Epoch 129/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4481 - val_loss: 0.5251 - lr: 0.0040\n",
            "Epoch 130/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4480 - val_loss: 0.5252 - lr: 0.0040\n",
            "Epoch 131/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4476 - val_loss: 0.5246 - lr: 0.0040\n",
            "Epoch 132/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4473 - val_loss: 0.5241 - lr: 0.0040\n",
            "Epoch 133/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4470 - val_loss: 0.5248 - lr: 0.0040\n",
            "Epoch 134/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4466 - val_loss: 0.5238 - lr: 0.0040\n",
            "Epoch 135/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4463 - val_loss: 0.5229 - lr: 0.0040\n",
            "Epoch 136/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4459 - val_loss: 0.5228 - lr: 0.0040\n",
            "Epoch 137/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4457 - val_loss: 0.5228 - lr: 0.0040\n",
            "Epoch 138/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4453 - val_loss: 0.5231 - lr: 0.0040\n",
            "Epoch 139/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4450 - val_loss: 0.5223 - lr: 0.0040\n",
            "Epoch 140/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4448 - val_loss: 0.5216 - lr: 0.0040\n",
            "Epoch 141/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4444 - val_loss: 0.5210 - lr: 0.0040\n",
            "Epoch 142/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4441 - val_loss: 0.5209 - lr: 0.0040\n",
            "Epoch 143/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4439 - val_loss: 0.5214 - lr: 0.0040\n",
            "Epoch 144/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4435 - val_loss: 0.5204 - lr: 0.0040\n",
            "Epoch 145/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4433 - val_loss: 0.5209 - lr: 0.0040\n",
            "Epoch 146/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4428 - val_loss: 0.5204 - lr: 0.0040\n",
            "Epoch 147/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4425 - val_loss: 0.5202 - lr: 0.0040\n",
            "Epoch 148/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4422 - val_loss: 0.5194 - lr: 0.0040\n",
            "Epoch 149/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4421 - val_loss: 0.5191 - lr: 0.0040\n",
            "Epoch 150/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4416 - val_loss: 0.5182 - lr: 0.0040\n",
            "Epoch 151/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4414 - val_loss: 0.5187 - lr: 0.0040\n",
            "Epoch 152/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4410 - val_loss: 0.5187 - lr: 0.0040\n",
            "Epoch 153/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4407 - val_loss: 0.5181 - lr: 0.0040\n",
            "Epoch 154/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4405 - val_loss: 0.5176 - lr: 0.0040\n",
            "Epoch 155/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4402 - val_loss: 0.5170 - lr: 0.0040\n",
            "Epoch 156/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4398 - val_loss: 0.5175 - lr: 0.0040\n",
            "Epoch 157/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4395 - val_loss: 0.5169 - lr: 0.0040\n",
            "Epoch 158/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4393 - val_loss: 0.5169 - lr: 0.0040\n",
            "Epoch 159/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4389 - val_loss: 0.5161 - lr: 0.0040\n",
            "Epoch 160/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4387 - val_loss: 0.5163 - lr: 0.0040\n",
            "Epoch 161/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4384 - val_loss: 0.5160 - lr: 0.0040\n",
            "Epoch 162/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4382 - val_loss: 0.5154 - lr: 0.0040\n",
            "Epoch 163/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4377 - val_loss: 0.5148 - lr: 0.0040\n",
            "Epoch 164/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4374 - val_loss: 0.5152 - lr: 0.0040\n",
            "Epoch 165/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4371 - val_loss: 0.5144 - lr: 0.0040\n",
            "Epoch 166/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4369 - val_loss: 0.5145 - lr: 0.0040\n",
            "Epoch 167/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4366 - val_loss: 0.5139 - lr: 0.0040\n",
            "Epoch 168/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4362 - val_loss: 0.5145 - lr: 0.0040\n",
            "Epoch 169/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4361 - val_loss: 0.5136 - lr: 0.0040\n",
            "Epoch 170/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4357 - val_loss: 0.5139 - lr: 0.0040\n",
            "Epoch 171/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4355 - val_loss: 0.5141 - lr: 0.0040\n",
            "Epoch 172/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4350 - val_loss: 0.5124 - lr: 0.0040\n",
            "Epoch 173/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4348 - val_loss: 0.5132 - lr: 0.0040\n",
            "Epoch 174/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4347 - val_loss: 0.5134 - lr: 0.0040\n",
            "Epoch 175/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4343 - val_loss: 0.5124 - lr: 0.0040\n",
            "Epoch 176/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4341 - val_loss: 0.5123 - lr: 0.0040\n",
            "Epoch 177/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4337 - val_loss: 0.5121 - lr: 0.0040\n",
            "Epoch 178/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4335 - val_loss: 0.5117 - lr: 0.0040\n",
            "Epoch 179/300\n",
            "156/156 [==============================] - 1s 5ms/step - loss: 0.4333 - val_loss: 0.5116 - lr: 0.0040\n",
            "Epoch 180/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4330 - val_loss: 0.5113 - lr: 0.0040\n",
            "Epoch 181/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4326 - val_loss: 0.5111 - lr: 0.0040\n",
            "Epoch 182/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4323 - val_loss: 0.5105 - lr: 0.0040\n",
            "Epoch 183/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4321 - val_loss: 0.5104 - lr: 0.0040\n",
            "Epoch 184/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4317 - val_loss: 0.5099 - lr: 0.0040\n",
            "Epoch 185/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4316 - val_loss: 0.5096 - lr: 0.0040\n",
            "Epoch 186/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4312 - val_loss: 0.5094 - lr: 0.0040\n",
            "Epoch 187/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4310 - val_loss: 0.5095 - lr: 0.0040\n",
            "Epoch 188/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4307 - val_loss: 0.5087 - lr: 0.0040\n",
            "Epoch 189/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4304 - val_loss: 0.5090 - lr: 0.0040\n",
            "Epoch 190/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4301 - val_loss: 0.5100 - lr: 0.0040\n",
            "Epoch 191/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4299 - val_loss: 0.5092 - lr: 0.0040\n",
            "Epoch 192/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4297 - val_loss: 0.5083 - lr: 0.0040\n",
            "Epoch 193/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4294 - val_loss: 0.5083 - lr: 0.0040\n",
            "Epoch 194/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4291 - val_loss: 0.5074 - lr: 0.0040\n",
            "Epoch 195/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4287 - val_loss: 0.5070 - lr: 0.0040\n",
            "Epoch 196/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4286 - val_loss: 0.5068 - lr: 0.0040\n",
            "Epoch 197/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4282 - val_loss: 0.5067 - lr: 0.0040\n",
            "Epoch 198/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4281 - val_loss: 0.5068 - lr: 0.0040\n",
            "Epoch 199/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4277 - val_loss: 0.5059 - lr: 0.0040\n",
            "Epoch 200/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4276 - val_loss: 0.5066 - lr: 0.0040\n",
            "Epoch 201/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4273 - val_loss: 0.5060 - lr: 0.0040\n",
            "Epoch 202/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4270 - val_loss: 0.5061 - lr: 0.0040\n",
            "Epoch 203/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4268 - val_loss: 0.5053 - lr: 0.0040\n",
            "Epoch 204/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4265 - val_loss: 0.5048 - lr: 0.0040\n",
            "Epoch 205/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4262 - val_loss: 0.5047 - lr: 0.0040\n",
            "Epoch 206/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4259 - val_loss: 0.5051 - lr: 0.0040\n",
            "Epoch 207/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4257 - val_loss: 0.5044 - lr: 0.0040\n",
            "Epoch 208/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4255 - val_loss: 0.5049 - lr: 0.0040\n",
            "Epoch 209/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4251 - val_loss: 0.5039 - lr: 0.0040\n",
            "Epoch 210/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4250 - val_loss: 0.5038 - lr: 0.0040\n",
            "Epoch 211/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4247 - val_loss: 0.5034 - lr: 0.0040\n",
            "Epoch 212/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4245 - val_loss: 0.5031 - lr: 0.0040\n",
            "Epoch 213/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4242 - val_loss: 0.5032 - lr: 0.0040\n",
            "Epoch 214/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4238 - val_loss: 0.5020 - lr: 0.0040\n",
            "Epoch 215/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4237 - val_loss: 0.5020 - lr: 0.0040\n",
            "Epoch 216/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4233 - val_loss: 0.5024 - lr: 0.0040\n",
            "Epoch 217/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4231 - val_loss: 0.5017 - lr: 0.0040\n",
            "Epoch 218/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4228 - val_loss: 0.5025 - lr: 0.0040\n",
            "Epoch 219/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4227 - val_loss: 0.5008 - lr: 0.0040\n",
            "Epoch 220/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4223 - val_loss: 0.5013 - lr: 0.0040\n",
            "Epoch 221/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4222 - val_loss: 0.5016 - lr: 0.0040\n",
            "Epoch 222/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4218 - val_loss: 0.5005 - lr: 0.0040\n",
            "Epoch 223/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4217 - val_loss: 0.5005 - lr: 0.0040\n",
            "Epoch 224/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4213 - val_loss: 0.5010 - lr: 0.0040\n",
            "Epoch 225/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4212 - val_loss: 0.5003 - lr: 0.0040\n",
            "Epoch 226/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4208 - val_loss: 0.5001 - lr: 0.0040\n",
            "Epoch 227/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4207 - val_loss: 0.5000 - lr: 0.0040\n",
            "Epoch 228/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4205 - val_loss: 0.4997 - lr: 0.0040\n",
            "Epoch 229/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4202 - val_loss: 0.4992 - lr: 0.0040\n",
            "Epoch 230/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4200 - val_loss: 0.4991 - lr: 0.0040\n",
            "Epoch 231/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4197 - val_loss: 0.4993 - lr: 0.0040\n",
            "Epoch 232/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4195 - val_loss: 0.4989 - lr: 0.0040\n",
            "Epoch 233/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4192 - val_loss: 0.4987 - lr: 0.0040\n",
            "Epoch 234/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4189 - val_loss: 0.4983 - lr: 0.0040\n",
            "Epoch 235/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4185 - val_loss: 0.4980 - lr: 0.0040\n",
            "Epoch 236/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4183 - val_loss: 0.4986 - lr: 0.0040\n",
            "Epoch 237/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4183 - val_loss: 0.4979 - lr: 0.0040\n",
            "Epoch 238/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4180 - val_loss: 0.4974 - lr: 0.0040\n",
            "Epoch 239/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4178 - val_loss: 0.4974 - lr: 0.0040\n",
            "Epoch 240/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4175 - val_loss: 0.4968 - lr: 0.0040\n",
            "Epoch 241/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4170 - val_loss: 0.4975 - lr: 0.0040\n",
            "Epoch 242/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4169 - val_loss: 0.4966 - lr: 0.0040\n",
            "Epoch 243/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4168 - val_loss: 0.4959 - lr: 0.0040\n",
            "Epoch 244/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4166 - val_loss: 0.4957 - lr: 0.0040\n",
            "Epoch 245/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4162 - val_loss: 0.4961 - lr: 0.0040\n",
            "Epoch 246/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4161 - val_loss: 0.4955 - lr: 0.0040\n",
            "Epoch 247/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4158 - val_loss: 0.4956 - lr: 0.0040\n",
            "Epoch 248/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4157 - val_loss: 0.4948 - lr: 0.0040\n",
            "Epoch 249/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4153 - val_loss: 0.4956 - lr: 0.0040\n",
            "Epoch 250/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4151 - val_loss: 0.4947 - lr: 0.0040\n",
            "Epoch 251/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4149 - val_loss: 0.4942 - lr: 0.0040\n",
            "Epoch 252/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4146 - val_loss: 0.4943 - lr: 0.0040\n",
            "Epoch 253/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4144 - val_loss: 0.4949 - lr: 0.0040\n",
            "Epoch 254/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4142 - val_loss: 0.4941 - lr: 0.0040\n",
            "Epoch 255/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4139 - val_loss: 0.4938 - lr: 0.0040\n",
            "Epoch 256/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4137 - val_loss: 0.4941 - lr: 0.0040\n",
            "Epoch 257/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4134 - val_loss: 0.4936 - lr: 0.0040\n",
            "Epoch 258/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4133 - val_loss: 0.4932 - lr: 0.0040\n",
            "Epoch 259/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4130 - val_loss: 0.4927 - lr: 0.0040\n",
            "Epoch 260/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4127 - val_loss: 0.4931 - lr: 0.0040\n",
            "Epoch 261/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4123 - val_loss: 0.4926 - lr: 0.0040\n",
            "Epoch 262/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4123 - val_loss: 0.4921 - lr: 0.0040\n",
            "Epoch 263/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4121 - val_loss: 0.4921 - lr: 0.0040\n",
            "Epoch 264/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4119 - val_loss: 0.4921 - lr: 0.0040\n",
            "Epoch 265/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4116 - val_loss: 0.4915 - lr: 0.0040\n",
            "Epoch 266/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4114 - val_loss: 0.4918 - lr: 0.0040\n",
            "Epoch 267/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4112 - val_loss: 0.4917 - lr: 0.0040\n",
            "Epoch 268/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4110 - val_loss: 0.4914 - lr: 0.0040\n",
            "Epoch 269/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4107 - val_loss: 0.4906 - lr: 0.0040\n",
            "Epoch 270/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4105 - val_loss: 0.4914 - lr: 0.0040\n",
            "Epoch 271/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4101 - val_loss: 0.4909 - lr: 0.0040\n",
            "Epoch 272/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4101 - val_loss: 0.4906 - lr: 0.0040\n",
            "Epoch 273/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4098 - val_loss: 0.4904 - lr: 0.0040\n",
            "Epoch 274/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4096 - val_loss: 0.4898 - lr: 0.0040\n",
            "Epoch 275/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4092 - val_loss: 0.4900 - lr: 0.0040\n",
            "Epoch 276/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4091 - val_loss: 0.4897 - lr: 0.0040\n",
            "Epoch 277/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4088 - val_loss: 0.4904 - lr: 0.0040\n",
            "Epoch 278/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4086 - val_loss: 0.4882 - lr: 0.0040\n",
            "Epoch 279/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4086 - val_loss: 0.4885 - lr: 0.0040\n",
            "Epoch 280/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4082 - val_loss: 0.4879 - lr: 0.0040\n",
            "Epoch 281/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4080 - val_loss: 0.4882 - lr: 0.0040\n",
            "Epoch 282/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4079 - val_loss: 0.4879 - lr: 0.0040\n",
            "Epoch 283/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4074 - val_loss: 0.4894 - lr: 0.0040\n",
            "Epoch 284/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4074 - val_loss: 0.4875 - lr: 0.0040\n",
            "Epoch 285/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4072 - val_loss: 0.4874 - lr: 0.0040\n",
            "Epoch 286/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4068 - val_loss: 0.4883 - lr: 0.0040\n",
            "Epoch 287/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4067 - val_loss: 0.4872 - lr: 0.0040\n",
            "Epoch 288/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4065 - val_loss: 0.4867 - lr: 0.0040\n",
            "Epoch 289/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4062 - val_loss: 0.4871 - lr: 0.0040\n",
            "Epoch 290/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4061 - val_loss: 0.4868 - lr: 0.0040\n",
            "Epoch 291/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4057 - val_loss: 0.4863 - lr: 0.0040\n",
            "Epoch 292/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4057 - val_loss: 0.4866 - lr: 0.0040\n",
            "Epoch 293/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4055 - val_loss: 0.4859 - lr: 0.0040\n",
            "Epoch 294/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4052 - val_loss: 0.4857 - lr: 0.0040\n",
            "Epoch 295/300\n",
            "156/156 [==============================] - 0s 3ms/step - loss: 0.4049 - val_loss: 0.4857 - lr: 0.0040\n",
            "Epoch 296/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4048 - val_loss: 0.4864 - lr: 0.0040\n",
            "Epoch 297/300\n",
            "156/156 [==============================] - 1s 4ms/step - loss: 0.4045 - val_loss: 0.4851 - lr: 0.0040\n",
            "Epoch 298/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4042 - val_loss: 0.4853 - lr: 0.0040\n",
            "Epoch 299/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4847 - lr: 0.0040\n",
            "Epoch 300/300\n",
            "156/156 [==============================] - 1s 3ms/step - loss: 0.4037 - val_loss: 0.4851 - lr: 0.0040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "zYO8QLKpmLJ1",
        "outputId": "c563d8ea-2199-4059-e460-b62fd5c85bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Classification"
      ],
      "metadata": {
        "id": "-S9gT2f19hAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf1 = LogisticRegression(multi_class='multinomial', random_state=0)\n",
        "clf2 = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "clf3 = GaussianNB()\n",
        "ensemble_classifier = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')"
      ],
      "metadata": {
        "id": "alPthfvQ9kOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_classifier = ensemble_classifier.fit(pca_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRXJ3TBi-H00",
        "outputId": "13df6af5-ae17-4f25-c8c6-1312d361d61a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(ensemble_classifier.predict(pca_test) == y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FonfKkvR_o5-",
        "outputId": "6a8fd22b-5571-4058-d5e0-fe47b2f263fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9923076923076923"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "I tried to improve the auto encoder by changing its architecture. \n",
        "I tried a deeper auto encoder and also a shallower one. \n",
        "Non of them improved the papers reported mean squared error. \n",
        "\n",
        "\n",
        "So I used an ensemble method for the classification part and improved the test accuracy to 99.2%."
      ],
      "metadata": {
        "id": "Ag_CL3PZ_t-C"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}